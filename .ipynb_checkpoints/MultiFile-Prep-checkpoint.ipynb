{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94be7c9f",
   "metadata": {},
   "source": [
    "## Multiple File Exploration and Analysis\n",
    "\n",
    "In the previous workbook, SingleFile, we used the FeatureReader to explore a single document. In the next two workbooks, \"Multifile-Prep\" and \"Multifile-Analysis\", we'll use the FeatureReader and Gensim to analyze a collection of documents and build a topic model.\n",
    "\n",
    "This workbook will prepare data for analysis, and Multifile-Analysis will load the data created in this worbook to build a topic model using Gensim. \n",
    "\n",
    "For this workshop, we'll continue to use a UCSF health sciences related dataset. However you should be able to swap in any of the sample datasets prepared by HathiTrust:\n",
    "\n",
    "https://analytics.hathitrust.org/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from htrc_features import FeatureReader, Volume\n",
    "import glob\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09128e09",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "We'll use glob to recusively find all .bz2 files available in the directory structure, then append each file to a list of paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1808a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "import glob\n",
    "for file in glob.glob('data/uc1/**/*.bz2', recursive=True):\n",
    "#for file in glob.glob('data/SOM-30Vol/uc1/**/*.bz2', recursive=True):\n",
    "    paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91040df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3fe53",
   "metadata": {},
   "source": [
    "### Prepare a dataset for a topic modeling exercise\n",
    "\n",
    "To illustrate topic modeling, we'll prepare a dataset consisting of records containing \"nursing\" or \"denistry\" in the title. Keep in mind, you don't need to do this to use topic modeling to analyze a dataset. For unsupervised machine learning, you don't need to pre-define your categories or topics - the algorithm will identify clusters of documents around potential topics of interest for you. However, for an exercise, it can help to have a sense of our topics as it will help us see how the algorithm is identifying clusters that emerge from the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e478ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a few minutes to run.\n",
    "# to make it run faster, you may want to lower the sample size\n",
    "\n",
    "sample_size = 20\n",
    "i = 0\n",
    "fr = FeatureReader(paths)\n",
    "nursing_count = 0\n",
    "dentistry_count = 0\n",
    "vols = []\n",
    "vol_row_id = []\n",
    "for vol in fr.volumes():\n",
    "    title = vol.title.lower()\n",
    "    if 'nursing' in title and nursing_count < sample_size:\n",
    "        vols.append(vol)\n",
    "        nursing_count += 1\n",
    "    if 'dentistry' in title and dentistry_count < sample_size:\n",
    "        vols.append(vol)\n",
    "        dentistry_count += 1\n",
    "    if dentistry_count >= 20 and nursing_count >= sample_size:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b36df5",
   "metadata": {},
   "source": [
    "### Volumes\n",
    "\n",
    "We can print the title for each volume in our collection by iterating over each element in the vols collection we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in vols:\n",
    "    print(v.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10839b07",
   "metadata": {},
   "source": [
    "### Individual record access\n",
    "\n",
    "We can access the metadata from the list for each record the same way we did with the single file case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62212a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing only the first 10 token lists\n",
    "for i, p in enumerate(vols[0].pages()):\n",
    "    print(i+1, p.tokenlist())\n",
    "    print()\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb707d",
   "metadata": {},
   "source": [
    "### Exercise: Take a look at some other records and get a sense of how text is extracted\n",
    "\n",
    "* how complete is it?\n",
    "* what do you gain from relying exclusively on word count? what do you lose?\n",
    "* is there clutter? Are non-alphanumeric characters useful to you?\n",
    "* what do you lose if you don't know the position or part of speech of words? \n",
    "* how could varying transcription thoroughness and accuracy influence your research?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14137480",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "vols[0].tokenlist(case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5914ccac",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "In the previous workbook, we discussed the potential \"clutter\" in our text. We may not want punctuation, non-alphanumeric characters, or stop words, and we may want to lemmatize or stem some of our terms\n",
    "\n",
    "A full review of cleaning text data is beyond the scope of this workshop, though we will remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623482f",
   "metadata": {},
   "source": [
    "### Create a bag of words\n",
    "\n",
    "This next bit of code is a little complicated. We're using some of the techniques discussed earlier for going line-by-line through the pages of each volume to create a bag of words model and term frequency vector for each page. \n",
    "\n",
    "For now, let's walk through this code, take a look at the output, and and discuss how it works. You may also want to refer back to the of bag-of-words visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - tdqm is used to provide a progress bar, since this bit of code can take a while to run.\n",
    "# it's optional but can be useful to get a sense of how your code is progressing and how much longer it will\n",
    "# take to run\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "vol_df = pd.DataFrame(columns=['htid', 'page_number', 'page_tokens'])\n",
    "\n",
    "for vol in tqdm(vols, total=len(vols)):\n",
    "    title = vol.title\n",
    "    htid = vol.id\n",
    "    for page in vol.pages():\n",
    "        page_num = str(page).split(' ')[1]\n",
    "        page_df = page.tokenlist(section='body', case=False, pos=False)\n",
    "        \n",
    "        tkn_list = []\n",
    "        \n",
    "        for i, r in page_df.iterrows():\n",
    "            #print(i[0], i[1], i[2])\n",
    "            word = i[2]\n",
    "            count = r[0]\n",
    "            #print(word, count)\n",
    "            word = word.strip()\n",
    "            \n",
    "            if word not in en_stop and word.isalpha() and len(word) > 2:\n",
    "                for _ in range(count):\n",
    "                    tkn_list.append(word)\n",
    "        \n",
    "        if len(tkn_list) > 50 and 'nursing' in tkn_list and 'dentistry' in tkn_list:   \n",
    "            app_df = pd.DataFrame({'title':title, 'htid': htid, 'page_number':  page_num, 'page_tokens': [tkn_list]})\n",
    "            #print(app_df)\n",
    "            vol_df = pd.concat([vol_df, app_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4dc75c",
   "metadata": {},
   "source": [
    "### Dataframe for Text Analysis\n",
    "\n",
    "The code above creates a dataframe with each document id, page number, the tokens (including duplicates) for each page, and the title for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd086ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27959a",
   "metadata": {},
   "source": [
    "### Pickle\n",
    "\n",
    "We've created our dataframe. We'll save it to a directory in our workspace and load it for analysis in the next workbook.\n",
    "\n",
    "You may be wondering why we aren't using the to_csv method to export our data to a csv. The reason we're doing it with pickle (which writes the model to disk but can't be read like a csv) is that our page_tokens column contains lists, rather than single values. This can cause issues when writing to/reading from csv files. There are ways around this, but it may be easier to pickle the data if you don't need to read it outside a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a04f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas to_csv will convert lists into strings, which will be a hassle.\n",
    "# better to pickle it. \n",
    "vol_df.to_pickle('processed_data/ucsf_medical.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e67c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca1d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
