{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8c2087",
   "metadata": {},
   "source": [
    "## Single File Exploration and Analysis\n",
    "\n",
    "We'll start by looking at a single file using the HTRC FeatureReader. This will give us an opportunity to explore the feature reader API on a smaller dataset before moving on to analysis of larger collections.\n",
    "\n",
    "Note: For this workshop, we're using a collection of documents from UCSF Health Sciences. This workshop is adapted from tutorial material and documentation from HathiTrust and Programming Historian. If you'd like to follow along with this workbook but don't have the UCSF dataset, you can (with some modifications) use sample datasets from HathiTrust. Links are below.\n",
    "\n",
    "Programming Historian Tutorial:\n",
    "    https://programminghistorian.org/en/lessons/text-mining-with-extracted-features\n",
    "    \n",
    "Sample Data:\n",
    "    https://analytics.hathitrust.org/datasets\n",
    "\n",
    "        \n",
    "HathiTrust FeatureReader documentation and examples\n",
    "    https://github.com/htrc/htrc-feature-reader\n",
    "    \n",
    "The Feature Reader provides an extensive set of NLP tools for text analysis, more than we can cover in this workshop. The goal here is to introduce you to this API and do enough programming with the FeatureReader that you feel familiar enough to continue reading, coding, and applying it to your research and projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4e806",
   "metadata": {},
   "source": [
    "### Import modules and set python and notebook parameters\n",
    "\n",
    "First, some setup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fa137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install htrc-feature-reader\n",
    "!pip install pyLDAvis\n",
    "!pip install pandas==1.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, Volume\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas\n",
    "pandas.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feca339",
   "metadata": {},
   "source": [
    "### Read in a single, multi-page volume\n",
    "\n",
    "We'll use the Volume interface to read in a single document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd265e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = Volume('data/uc1/30268/uc1.32106020265887.json.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84972a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b119d",
   "metadata": {},
   "source": [
    "Jupyter Notebook will provide some formatting and create links to this document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70e95b",
   "metadata": {},
   "source": [
    "You can also access individual attributes using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)\n",
    "print(vol.id, vol.page_count, vol.year, vol.language, vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b2078",
   "metadata": {},
   "source": [
    "To get a list of all attributes available on a Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250239d5",
   "metadata": {},
   "source": [
    "vol.parser.meta.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e6da0",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Take a look at and familiarize yourself with the article. Think about how text could be extracted from it. What information, in addition to text, would you want to preserve for your research? What information loss might you experience if you rely purely on text extraction? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adb097",
   "metadata": {},
   "source": [
    "### The Volume interface\n",
    "\n",
    "In this next section, we'll take a short tour of the Volume interface. The API is more extensive than this, so you're encouraged to keep exploring after this workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a73d0",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "The concept of a token is key to using the Volume API. \n",
    "\n",
    "\"A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\"\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d371d3",
   "metadata": {},
   "source": [
    "### Token Counts\n",
    "\n",
    "We can use tokens_per_page() to count and visualize the number of tokens per page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c532d49",
   "metadata": {},
   "source": [
    "vol.tokens_per_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a71a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "tokens = vol.tokens_per_page()\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea95a3",
   "metadata": {},
   "source": [
    "### Unique Tokens\n",
    "\n",
    "To get unique tokens for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = vol.tokens()\n",
    "\n",
    "# convert to a list to display only the first 10\n",
    "list(unique_tokens)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff658932",
   "metadata": {},
   "source": [
    "### Token List\n",
    "\n",
    "The Extracted Features dataset also provides token counts with much more granularity (page, section, token, part of speect, and count for each token)\n",
    "\n",
    "Parts of speech use Penn tree banking:\n",
    "    \n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74645021",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.tokenlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a70abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a groupby to count the number of each part of speech\n",
    "tl.groupby(level=[\"pos\"]).sum()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359412f",
   "metadata": {},
   "source": [
    "### Page and Parameters\n",
    "\n",
    "You can access tokens for a specific page, and specify certain parameters (ignore case, select for specific part of speech or section of the document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.tokenlist(page_select=9,case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21b332",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try out a few other methods available on a Volume. You can go to the API or try out the list below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.line_counts()\n",
    "#vol.sentence_counts()\n",
    "#vol.empty_line_counts()\n",
    "#vol.begin_line_chars()\n",
    "#vol.end_line_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.line_counts().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f35ca1",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Take a look at some other records and get a sense of how text is extracted\n",
    "* how complete is it?\n",
    "* what do you gain from relying exclusively on word count? what do you lose?\n",
    "* is there clutter? Are non-alphanumeric characters useful to you?\n",
    "* how could it help to know the position or part of speech of a token? \n",
    "* how could varying transcription thoroughness and accuracy influence your research?\n",
    "\n",
    "http://htrc.github.io/htrc-feature-reader/htrc_features/feature_reader.m.html#htrc_features.feature_reader.Volume.tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2913e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_rows', None)\n",
    "vol.tokenlist(drop_section=False,case=False, pos=False)[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0085df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select counts of the word ‘academic’ for all pages and all page sections (first 10 results)\n",
    "tl.loc[(slice(None), slice(None), \"academic\"),][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b9e60",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try to find the word “nursing” in this record, and compare where that shows up to the token-per-page pattern previously plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc364894",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_nursing = vol.tokenlist()\n",
    "nursing_pages = tl_nursing.loc[(slice(None), slice(None), \"nursing\"),]\n",
    "nursing_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dc3a7",
   "metadata": {},
   "source": [
    "### Counting and sorting\n",
    "\n",
    "For the next exercise, we'll limit our analysis to one page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying first 10\n",
    "vol.tokenlist(page_select=8,case=False).sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca8c76",
   "metadata": {},
   "source": [
    "### Filtering based on token count\n",
    "\n",
    "Use the \"boolean mask\" technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ce279",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_page = vol.tokenlist(page_select=8,case=False)\n",
    "pandas.set_option('display.max_rows', None)\n",
    "tl_page[tl_page['count'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91877f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can order them and print the first n results\n",
    "tl_page.sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc073c0",
   "metadata": {},
   "source": [
    "### Something to Consider\n",
    "\n",
    "The most common words, by count, are \"stop words\", common words that might not be useful in an analysis.\n",
    "In the next section, where we consider a collection of documents, we'll review a technique to remove stop words prior to analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eddc95",
   "metadata": {},
   "source": [
    "### Working with granular, row-by-row data\n",
    "\n",
    "At some point, you may want to write your own code to work on a granular level with the contents of a record, rather than relying on dataframe operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break to print only the first pate\n",
    "\n",
    "for page in vol.pages():\n",
    "    print('page:', page)\n",
    "    \n",
    "    page_df = page.tokenlist()\n",
    "\n",
    "    for i, r in page_df.iterrows():\n",
    "        print('i:', i)\n",
    "        print('r:', r)\n",
    "        print()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422cc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d572a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
